{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft\n",
    "\n",
    "# Statistical Machine Translation Using IBM Translation Models\n",
    "\n",
    "An ongoing project to translate English to French.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The IBM translation models are a family of statistical machine translation algorithms that date to the late 1980s. The models were created as part of IBM's \"Candide\" project, which aimed to automatically translate French to English. As an excercise in theory, I decided to try implementing the models myself. The goal of this implementation is efficiency: I want to build a learning algorithm that can be trained even on home computers.\n",
    "\n",
    "\n",
    "The IBM models work by estimating the conditional probability that a given sentence in one language is the translation of a given  sentence in the other. Learning a distribution like this is difficult because training data is generally sparse. An algorithm is unlikely to see the same string translated two different ways, and unless one vectorizes sentences very cleverly, there is little relationship between the Euclidian distance of two string-vectors and their actual semantic similarity. \n",
    "\n",
    "Clever vectorization was not the strategy used by Candide team. Rather, they chose to make strict assumptions about the range of distributions the translation model would consider. With these restrictions in place, they then performed Maximum Likelihood Estimation to choose the likeliest such distribution.\n",
    "\n",
    "A consequence of this approach is that, under the restrictions imposed by the Candide team, even the likeliest distribution tended to assign high conditional probabilities to ill-formed sentences. To resolve this, the Candide team adopted a noisy channel approach to translation. That is, rather than estimate directly the probabability \n",
    "\n",
    "$$\\mathbb{P}(\\mathbf{e}|\\mathbf{f})$$\n",
    "\n",
    "that an English string $\\mathbf{e}$ is the translation of a French string $\\mathbf{f}$, they chose rather to estimate the Bayesian equivalent\n",
    "\n",
    "$$\\frac{\\mathbb{P}(\\mathbf{e})\\mathbb{P}(\\mathbf{f}|\\mathbf{e})}{\\mathbb{P}(\\mathbf{f})}.$$\n",
    "\n",
    "One translates an English sentence $\\mathbf{e}$ to the French sentence $\\mathbf{f}$ that maximizes the product\n",
    "\n",
    "$$\\mathbb{P}(\\mathbf{e})\\mathbb{P}(\\mathbf{f}|\\mathbf{e}).$$\n",
    "\n",
    "The Candide team playfully described their reasoning as follows:\n",
    ">A string of English words, $\\mathbf{e}$, can be translated into a string of French words in many different ways. Often, knowing the broader context in which $\\mathbf{e}$ occurs may serve to winnow the field of acceptable French translations, but even so, many acceptable translations will remain; the choice among them is largely a matter of taste. In statistical translation, we take the view than every French string, $\\mathbf{f}$, is a possible translation of $\\mathbf{e}$. We assign to every pair of strings $(\\mathbf{e}, \\mathbf{f})$ a number $\\mathbb{P}(\\mathbf{e}|\\mathbf{f})$, which we interpret as the probability that a translator, when presented with $\\mathbf{e}$, will produce $\\mathbf{f}$ as his translation. We further take the view that when a native speaker of French produces a string of French words, he has actually concieved of a string of English words, which he translated mentally. Given a French string $\\mathbf{f}$, the job of our translation sustem is to find the string $\\mathbf{e}$ that the native speaker had in mind then he produced $\\mathbf{f}$. We minimize our chance of error by choosing that string $\\hat{\\mathbf{e}}$ for which $\\mathbb{P}(\\mathbf{e} | \\mathbf{f})$ is greatest. (Brown et. al. 1993)\n",
    "\n",
    "Practically, this approach shifts the emphasis on producing well-formed English strings to the distribution $\\mathbb{P}(\\mathbf{e})$, which is estimated by a \"language model.\" A good language model assigns very low probabilites to ill-formed strings. The distribution $\\mathbb{P}(\n",
    "\\mathbf{f} | \\mathbf{e})$ is estimated by a \"translation model\", of which the IBM models are an example.\n",
    "\n",
    "## IBM Model 1\n",
    "\n",
    "My implementation of IBM's simplest translation model is contained in the notebook \"IBM Model 1.ipynb\". Model 1 assumes that the probability $\\mathbb{P}(\\mathbf{f} | \\mathbf{e})$ is a conditional distribution of the form\n",
    "\n",
    "$$\\mathbb{P}(\\mathbf{f} | \\mathbf{e}) = \\epsilon(m|l)\\prod_{f \\in \\mathcal{F}} \\left(\\sum_{e \\in \\mathcal{E}} c(e, \\mathbf{e})t(f|e)\\right)^{c(f, \\mathbf{f})},$$\n",
    "\n",
    "where\n",
    "- $\\mathcal{E}$ is the set of all English words,\n",
    "- $\\mathcal{F}$ is the set of all French words, \n",
    "- the function $c(w, \\mathbf{w})$ counts how many times a word w shows up in a string $\\mathbf{w}$\n",
    "- the conditional distribution $t(f|e)$ estimates the probability that a French word $f$ is the translation of an English word $e$,\n",
    "- the conditional distribution $\\epsilon(m|l)$ estimates the probability that an English word of length $l$ translates to a French word fo length $m$.\n",
    "\n",
    "This model can be justified by the following assumptions. The probability that an English string $\\mathbf{e}$ translates to a French string containing a French word $f$ is a conditional distibution of the form \n",
    "\n",
    "$$\\mathbb{P}(f | \\mathbf{e}) = \\sum_{e \\in \\mathcal{E}} c(e, \\mathbf{e})t(f|e).$$\n",
    "\n",
    "We imagine that translation of $\\mathbf{e}$ into $\\mathbf{f}$ is a sequence of independent trials. First, a length $m$ is chosen according to the conditional distribution $\\epsilon(m|l)$. Next, $m$ french words are independently drawn from the conditional distribution $\\mathbb{P}(f | \\mathbf{e}).$\n",
    "\n",
    "Every distribution $\\mathbb{P}(\\mathbf{f}|\\mathbf{e})$ of this form is uniquely determined by the distributions $\\epsilon$ and $t$. Our choice of $\\epsilon$ and $t$ by a sample $S$ we draw of $n$ translated strings $(\\mathbf{e}, \\mathbf{f})$. Given such a sample, we choose $\\epsilon$ and $t$ to maximize the cross-entropy\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{(\\mathbf{e}, \\mathbf{f}) \\in S}\\ln(\\mathbb{P}(\\mathbf{f}|\\mathbf{e})) = \\frac{1}{n} \\sum_{(\\mathbf{e}, \\mathbf{f}) \\in S} \\left( \\ln(\\epsilon(m|l)) + \\sum_{f \\in \\mathcal{F}}c(f, \\mathbf{f})\\ln \\left( \\sum_{e \\in \\mathcal{E}} c(e, \\mathbf{e})t(f|e) \\right)\\right) .$$\n",
    "\n",
    "For those unfamiliar with cross-entropy, this may seem an odd choice of gain function. We justify it as follows. We can define a distribution $\\hat{\\mu}$ over english strings with the formula\n",
    "\n",
    "$$\\hat{\\mu}(\\mathbf{e}) = \\frac{c(\\mathbf{e}, S)}{n}$$,\n",
    "\n",
    "where $c(\\mathbf{e}, S)$ is the number of times the string $\\mathbf{e}$ appears in the sample $S$. We will call $\\hat{\\mu}$ the empirical marginal distribution of our sample $S$. Similarly, we can define a conditional distribution $K$ from the set of all English strings to the set of all French strings by\n",
    "\n",
    "$$ K(\\mathbf{f} | \\mathbf{e}) = \\frac{c((\\mathbf{e}, \\mathbf{f}), S)}{c(\\mathbf{e}, S)},$$ \n",
    "\n",
    "where $c((\\mathbf{e}, \\hat{\\mathbf{f}}), S)$ is the number of times the pair $(\\mathbf{e}, \\hat{\\mathbf{f}})$ appears in the sample $S$. We wll call $K$ the empirical kernel of $S$.\n",
    "\n",
    "Given two distributions $\\mu$, $\\nu$ over some set, with $\\mu$ absolutely continuous with respect to $\\nu$, the Kullback-Liebler divergence $D(\\mu || \\nu)$ is a measure of distance between the two distributions. It is given by \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
